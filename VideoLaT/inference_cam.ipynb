{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809e16fd",
   "metadata": {},
   "source": [
    "# Hand Gesture Transformer – Inference notebook\n",
    "\n",
    "훈련된 checkpoint를 불러와 실제 이미지를 Mediapipe로 전처리한 뒤 제스처를 분류합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538921fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "from config import GESTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────── 모델 로드 ─────────\n",
    "def load_model(path=\"./checkpoint/best.pth\", device=\"cpu\"):\n",
    "    model = torch.load(path, map_location=torch.device(device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ───────── MediaPipe 초기화 ─────────\n",
    "def init_mediapipe():\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(max_num_hands=1,\n",
    "                           min_detection_confidence=0.6,\n",
    "                           min_tracking_confidence=0.6)\n",
    "    return mp_hands, hands\n",
    "\n",
    "# ───────── 프레임 → 21×3 랜드마크 벡터 ─────────\n",
    "def extract_joint_vector(frame, hands, mp_hands):\n",
    "    frame_rgb = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "    res = hands.process(frame_rgb)\n",
    "    if res.multi_hand_landmarks:\n",
    "        lm = res.multi_hand_landmarks[0]          # 첫 손만 사용\n",
    "        joint = np.array([[p.x, p.y, p.z] for p in lm.landmark], dtype=np.float32)\n",
    "        return joint                              # (21,3)\n",
    "    return None\n",
    "\n",
    "# ───────── 30프레임 시퀀스 → 추론 ─────────\n",
    "def predict(model, seq, device=\"cpu\"):\n",
    "    if len(seq) < 30:\n",
    "        return None, None\n",
    "    x = np.stack(seq[-30:], axis=0)               # (30,21,3)\n",
    "    x = torch.from_numpy(x).unsqueeze(0)          # (1,30,21,3)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x.to(device))\n",
    "        conf, idx = torch.max(torch.softmax(logits, dim=1), dim=1)\n",
    "    return conf.item(), idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./checkpoint/best.pth\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device={device}\")\n",
    "model = load_model(model_path=model_path, device=device)\n",
    "mp_hands, hands = init_mediapipe()\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "seq = deque(maxlen=30)                       # 최신 30프레임 보관\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        joint = extract_joint_vector(frame, hands, mp_hands)\n",
    "        if joint is not None:\n",
    "            seq.append(joint)\n",
    "            conf, idx = predict(model, list(seq), device)\n",
    "            if conf and conf >= 0.8:\n",
    "                gesture = GESTURE[idx]\n",
    "                print(f\"{gesture} ({conf:.2f})\")\n",
    "\n",
    "        cv2.imshow(\"Webcam\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
