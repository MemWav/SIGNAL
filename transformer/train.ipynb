{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158d89c1",
   "metadata": {},
   "source": [
    "# Hand Gesture Transformer – Training notebook\n",
    "\n",
    "이 노트북은 Mediapipe로 추출한 21개 손 랜드마크(3‑D)를 Transformer Encoder에 넣어 4‑클래스 손동작을 분류하는 모델을 학습합니다.\n",
    "\n",
    "* 데이터 구성: `dataset/<gesture_label>/*.jpg`\n",
    "* Train : Val : Test = 70 : 20 : 10\n",
    "* 결과: 에포크별 loss 그래프, Attention heat‑map 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d56fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Colab 환경이라면) 필요 패키지 설치\n",
    "# !pip install mediapipe nbformat torch torchvision matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, glob, json, math, itertools, time, copy\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from models.HandGestureTransformer import HandGestureTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- 데이터셋 정의 -------\n",
    "GESTURES = ['gesture0', 'gesture1', 'gesture2', 'gesture3']  # 수정 가능\n",
    "\n",
    "class HandDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', val_ratio=0.2, test_ratio=0.1, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.mp_hands = mp.solutions.hands.Hands(static_image_mode=True,\n",
    "                                                 max_num_hands=1,\n",
    "                                                 min_detection_confidence=0.5)\n",
    "        random.seed(42)\n",
    "        # Gather all (img_path, label)\n",
    "        all_items = []\n",
    "        for idx, g in enumerate(GESTURES):\n",
    "            for p in glob.glob(os.path.join(root_dir, g, '*')):\n",
    "                all_items.append((p, idx))\n",
    "        random.shuffle(all_items)\n",
    "        n = len(all_items)\n",
    "        n_val = int(n * val_ratio)\n",
    "        n_test = int(n * test_ratio)\n",
    "        if split == 'train':\n",
    "            self.items = all_items[: n - n_val - n_test]\n",
    "        elif split == 'val':\n",
    "            self.items = all_items[n - n_val - n_test : n - n_test]\n",
    "        else:\n",
    "            self.items = all_items[-n_test:]\n",
    "\n",
    "    def _extract_landmarks(self, img_bgr):\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        res = self.mp_hands.process(img_rgb)\n",
    "        if res.multi_hand_landmarks:\n",
    "            lm = res.multi_hand_landmarks[0]\n",
    "            h, w, _ = img_bgr.shape\n",
    "            xyz = [(pt.x, pt.y, pt.z) for pt in lm.landmark]\n",
    "            return np.array(xyz, dtype=np.float32)\n",
    "        else:\n",
    "            return np.zeros((21,3), dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.items[idx]\n",
    "        img = cv2.imread(path)\n",
    "        xyz = self._extract_landmarks(img)\n",
    "        if self.transform: xyz = self.transform(xyz)\n",
    "        return torch.tensor(xyz), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00342bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 학습 루프 --------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'{device}\\n')\n",
    "root_dir = './dataset'  # 데이터 위치 수정\n",
    "batch = 256\n",
    "ds_train = HandDataset(root_dir, 'train')\n",
    "ds_val   = HandDataset(root_dir, 'val')\n",
    "train_loader = DataLoader(ds_train, batch_size=batch, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(ds_val, batch_size=batch, shuffle=False, num_workers=4)\n",
    "\n",
    "model = HandGestureTransformer(return_attn=True).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "EPOCHS = 40\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "best_val = float('inf')\n",
    "best_path = 'ckpt_best.pt'\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    tloss = 0\n",
    "    for xyz, label in train_loader:\n",
    "        xyz, label = xyz.to(device), label.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(xyz)\n",
    "        loss = crit(out, label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tloss += loss.item()*xyz.size(0)\n",
    "    train_losses.append(tloss/len(ds_train))\n",
    "\n",
    "    model.eval(); vloss=0\n",
    "    with torch.no_grad():\n",
    "        for xyz,label in val_loader:\n",
    "            xyz, label = xyz.to(device), label.to(device)\n",
    "            vloss += crit(model(xyz), label).item()*xyz.size(0)\n",
    "    val_loss = vloss / len(ds_val)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch}: train {train_losses[-1]:.4f}  val {val_losses[-1]:.4f}\")\n",
    "    torch.save({'epoch':epoch,'model':model.state_dict()}, f'ckpt_{epoch}.pt')\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model': model.state_dict()},\n",
    "                    best_path)\n",
    "        print(f'★ New best -> epoch: {epoch}, val loss: {val_loss:.4f}')\n",
    "\n",
    "# -------- Loss Plot --------\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='train'); plt.plot(val_losses, label='val')\n",
    "plt.xlabel('epoch'); plt.ylabel('loss'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Attention Heat‑map ---------\n",
    "sample_xyz, _ = ds_val[0]\n",
    "sample_logits = model(sample_xyz.unsqueeze(0).to(device))\n",
    "attn_weights = torch.stack(model.attn_maps)  # [L,B,nH,Len,Len]\n",
    "layer0_head0 = attn_weights[0,0,0].cpu().numpy()\n",
    "sns.heatmap(layer0_head0, cmap='viridis')\n",
    "plt.title('Layer0-Head0 Attention'); plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
