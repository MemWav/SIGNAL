{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809e16fd",
   "metadata": {},
   "source": [
    "# Hand Gesture Transformer – Inference notebook\n",
    "\n",
    "훈련된 checkpoint를 불러와 실제 이미지를 Mediapipe로 전처리한 뒤 제스처를 분류합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538921fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "from models.HandGestureTransformer import HandGestureTransformer\n",
    "from models.img_to_landmarks import img_to_landmarks\n",
    "\n",
    "# 모델 클래스 (학습 노트북과 동일)\n",
    "class HandGestureTransformer(nn.Module):\n",
    "    def __init__(self, d_model=128, num_layers=4, num_heads=8, n_gestures=4):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(3, d_model)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,d_model))\n",
    "        self.pos = nn.Parameter(torch.randn(1,22,d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model, num_heads, 4*d_model, dropout=0.1, batch_first=True)\n",
    "            for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, n_gestures)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        B = xyz.shape[0]\n",
    "        x = torch.cat([self.cls_token.expand(B,-1,-1), self.proj(xyz)],1) + self.pos\n",
    "        for layer in self.layers: x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        return self.head(x[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoint/ckpt_best.pt'  # 경로 수정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = HandGestureTransformer().to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device)['model'])\n",
    "model.eval()\n",
    "mp_hands = mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=1)\n",
    "GESTURES = ['gesture0','gesture1','gesture2','gesture3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_landmarks(img_path, mp_hands):\n",
    "    img_bgr = cv2.imread(img_path)\n",
    "    res = mp_hands.process(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "    if not res.multi_hand_landmarks:\n",
    "        raise ValueError('No hand detected')\n",
    "    lm = res.multi_hand_landmarks[0]\n",
    "    xyz = np.array([[pt.x, pt.y, pt.z] for pt in lm.landmark], dtype=np.float32)\n",
    "    return torch.tensor(xyz).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'assets/example1.png'\n",
    "\n",
    "xyz = img_to_landmarks(img_path).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(xyz)\n",
    "prob = torch.softmax(logits, -1).cpu().numpy()[0]\n",
    "for i,p in enumerate(prob):\n",
    "    print(f'{GESTURES[i]}: {p:.4f}')\n",
    "print('Predicted:', GESTURES[int(prob.argmax())])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
